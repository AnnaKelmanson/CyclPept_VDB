{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, DataStructs\n",
    "from rdkit.DataStructs.cDataStructs import ConvertToNumpyArray\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from rdkit.Chem.Scaffolds.MurckoScaffold import MurckoScaffoldSmiles, MurckoScaffoldSmilesFromSmiles\n",
    "from rdkit.DataStructs.cDataStructs import ExplicitBitVect, TanimotoSimilarity\n",
    "import chromadb\n",
    "from chromadb import Client, Settings, EmbeddingFunction\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage \n",
    "import statistics\n",
    "import random\n",
    "from langchain_community.chat_models import ChatOllama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "radius=3\n",
    "nBits=2048\n",
    "llm_model = 'gpt-3.5-turbo'\n",
    "num_llm_calls = 3\n",
    "context_window = 20\n",
    "template = \"{context}. As a language model you have an ability to understand this data based on the data provided, we do not need to build a regression model for a Language model Estimate. You can find relationships and make estimates. Make an estimate for Permeabilty based on SMILES and list of amino acids and on previous data for this row of data {query}. Report the predicted permeability value in the following format: PERMEABILITY(%value).\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_morgan_fingerprint(smiles, radius=radius, nBits=nBits):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    morgan_fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits)\n",
    "    arr = np.zeros((1,), dtype=np.int8)\n",
    "    ConvertToNumpyArray(morgan_fp, arr)\n",
    "    return arr.tolist()\n",
    "\n",
    "\n",
    "def find_smiles(text):\n",
    "    pattern = r'SMILES{([^}]*)}'\n",
    "    match = re.search(pattern, text)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def chemical_embedding(text):\n",
    "    return get_morgan_fingerprint(find_smiles(text), radius, nBits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ChemicalEmbeddingFunction(EmbeddingFunction):\n",
    "    def __call__(self, input):\n",
    "        smiles = self.find_smiles(input)\n",
    "        if smiles:\n",
    "            return self.get_morgan_fingerprint(smiles)\n",
    "        return None\n",
    "    \n",
    "    def embed_query(self, query):\n",
    "        # This method should handle the embedding of queries\n",
    "        smiles = self.find_smiles(query)\n",
    "        if smiles:\n",
    "            return self.get_morgan_fingerprint(smiles)\n",
    "        return None\n",
    "\n",
    "    def find_smiles(self, text):\n",
    "        pattern = r'SMILES{([^}]*)}'\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def get_morgan_fingerprint(self, smiles, radius=radius, nBits=nBits):\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        morgan_fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits)\n",
    "        arr = np.zeros((1,), dtype=np.int8)\n",
    "        ConvertToNumpyArray(morgan_fp, arr)\n",
    "        return arr.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_aldehydes_to_acids(smiles_list):\n",
    "    rxn_smarts = '[CX3H1:1](=O)[H].[OH2:2]>>[CX3:1](=O)[O:2]'\n",
    "    rxn = AllChem.ReactionFromSmarts(rxn_smarts)\n",
    "    \n",
    "    acid_smiles_list = []\n",
    "    for smiles in smiles_list:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        mol = Chem.AddHs(mol)\n",
    "        ps = rxn.RunReactants((mol, Chem.MolFromSmiles('O')))\n",
    "        if ps:\n",
    "            product_mol = ps[0][0]  \n",
    "            Chem.SanitizeMol(product_mol)  \n",
    "            acid_smiles = Chem.MolToSmiles(Chem.RemoveHs(product_mol), isomericSmiles=True, canonical=True)\n",
    "            acid_smiles_list.append(acid_smiles)\n",
    "        else:\n",
    "            print('Issue with converting C=O into -COOH')\n",
    "            acid_smiles_list.append(smiles)\n",
    "    \n",
    "    return acid_smiles_list\n",
    "\n",
    "def tokenize_peptides(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    pat = Chem.MolFromSmarts('NC=O')\n",
    "    \n",
    "    # Finding the largest ring\n",
    "    def find_largest_ring(mol):\n",
    "        sssr = Chem.GetSymmSSSR(mol)\n",
    "        largest_ring = max(sssr, key=len)\n",
    "        return set(largest_ring)\n",
    "\n",
    "    largest_ring = find_largest_ring(mol)\n",
    "    matches = mol.GetSubstructMatches(pat)\n",
    "    \n",
    "    emol = Chem.EditableMol(mol)\n",
    "\n",
    "    bonds_to_break = []\n",
    "\n",
    "    for match in matches:\n",
    "        N_idx, C_idx, O_idx = match\n",
    "        if N_idx in largest_ring and C_idx in largest_ring:\n",
    "            bonds_to_break.append((N_idx, C_idx))\n",
    "\n",
    "    # Break bonds \n",
    "    for N_idx, C_idx in sorted(bonds_to_break, reverse=True):  # Sort and reverse to avoid indexing issues\n",
    "        emol.RemoveBond(N_idx, C_idx)\n",
    "\n",
    "    fragmented_mol = emol.GetMol()\n",
    "    frags = Chem.GetMolFrags(fragmented_mol, asMols=True, sanitizeFrags=True)\n",
    "    fragment_smiles = [Chem.MolToSmiles(frag) for frag in frags]\n",
    "\n",
    "    return convert_aldehydes_to_acids(fragment_smiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_csv('../data/train_set.csv')\n",
    "test_set = pd.read_csv('../data/test_set.csv')\n",
    "val_set = pd.read_csv('../data/val_set.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chemical VDB generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ChemicalEmbeddingGenerator:\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_embedding(chunk):\n",
    "        def extract_smiles(text):\n",
    "            pattern = r'SMILES{([^}]*)}'\n",
    "            match = re.search(pattern, text)\n",
    "            if match:\n",
    "                return match.group(1)\n",
    "            return None\n",
    "        smiles_string = extract_smiles(chunk)\n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(smiles_string)\n",
    "            if mol is not None:\n",
    "                morgan_fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius=radius, nBits=nBits)\n",
    "                arr = np.zeros((1,), dtype=np.int8)\n",
    "                ConvertToNumpyArray(morgan_fp, arr)\n",
    "                #print(\"Embedding generated successfully.\")\n",
    "                return arr.tolist()\n",
    "            else:\n",
    "                print(\"Invalid SMILES string.\")\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while generating embedding: {e}\")\n",
    "            return None\n",
    "        \n",
    "\n",
    "\n",
    "class DataFrameManager:\n",
    "    def __init__(self):\n",
    "        self.embedding_generator = ChemicalEmbeddingGenerator()\n",
    "        self.chroma_client = chromadb.chromadb.Client()\n",
    "        self.collection_name = \"chemical_data_collection\"\n",
    "        self.collection = self.chroma_client.get_or_create_collection(name=self.collection_name,\n",
    "                                                                      metadata={\"hnsw:space\": \"cosine\"})\n",
    "        \n",
    "    def add_texts_to_collection(self, dataframe, text_column, columns_to_keep):\n",
    "        embeddings_list = []\n",
    "        documents_list = []\n",
    "        metadatas_list = []\n",
    "        ids_list = []\n",
    "        \n",
    "        for index, row in dataframe.iterrows():\n",
    "            embedding = self.embedding_generator.generate_embedding(row[text_column])\n",
    "            if embedding is not None:\n",
    "                unique_id = f\"row_{index}\"\n",
    "                embeddings_list.append(embedding)\n",
    "                documents_list.append(row[text_column])\n",
    "\n",
    "                metadata = {column: row[column] for column in columns_to_keep if row[column] is not None}\n",
    "                \n",
    "                metadatas_list.append(metadata)\n",
    "                ids_list.append(unique_id)\n",
    "        \n",
    "        self.collection.add(\n",
    "            embeddings=embeddings_list,\n",
    "            documents=documents_list,\n",
    "            metadatas=metadatas_list,\n",
    "            ids=ids_list\n",
    "        )\n",
    "        print(\"Data added to collection successfully.\")\n",
    "        return self.collection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context(query_dict_output):\n",
    "    context=''\n",
    "    for instance in query_dict_output['documents'][0]:\n",
    "        context+=instance + '. '\n",
    "    return context\n",
    "\n",
    "def get_distances(query_dict_output):\n",
    "    return query_dict_output['distances'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_value(input_string):\n",
    "    match = re.search(r'PERMEABILITY\\(([^)]+)\\)', input_string)\n",
    "\n",
    "    if match:\n",
    "        # Extracting the value\n",
    "        permeability_value = match.group(1)\n",
    "    else:\n",
    "        permeability_value = None\n",
    "\n",
    "    return float(permeability_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_values_from_context(input_string):\n",
    "\n",
    "    match = re.search(r'permeability value is ([\\-\\d\\.]+)', input_string)\n",
    "    permeability_value = match.group(1)\n",
    "\n",
    "    return float(permeability_value)\n",
    "\n",
    "def context_values(context_dict):\n",
    "    permeability = []\n",
    "    for sample in context_dict['documents'][0]:\n",
    "        permeability.append(get_values_from_context(sample))\n",
    "    return permeability\n",
    "\n",
    "def context_stats(context_permeability_list):\n",
    "    ''' The first value is mean, the second one is the closest by Tanimoto structure'''\n",
    "    return statistics.fmean(context_permeability_list), context_permeability_list[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest(target, numbers):\n",
    "    return min(numbers, key=lambda x: abs(x - target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_errors(exact_value, approximate_value):\n",
    "    absolute_error = abs(exact_value - approximate_value)\n",
    "    if exact_value != 0:  \n",
    "        relative_error = (absolute_error / abs(exact_value)) \n",
    "    else:\n",
    "        relative_error = None  \n",
    "    return round(relative_error, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(exact_value, approximate_value):\n",
    "    if exact_value != 0:  \n",
    "        accuracy = (1 - abs(exact_value - approximate_value) / abs(exact_value))\n",
    "    else:\n",
    "        accuracy = None\n",
    "    return round(accuracy, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_initial_data(row):\n",
    "    data = {\n",
    "        'text': [row['Formatted_String']],\n",
    "        'ground_truth': [row['PAMPA']],\n",
    "        'tanimoto': [row['Mean_Tanimoto_Similarity']],\n",
    "        'structure': [row['SMILES']],\n",
    "        'aminoacids': [row['tokens']],\n",
    "    }\n",
    "    return data\n",
    "\n",
    "def vdb_initialization(text):\n",
    "    sample_from_chroma = manager.collection.query(query_embeddings=[chemical_embedding(text)], n_results=context_window)\n",
    "    return sample_from_chroma\n",
    "\n",
    "def context_metrics(vdb_instance):\n",
    "    retrieved_values = context_values(vdb_instance)\n",
    "    context_mean_perm, context_top_1_perm = context_stats(retrieved_values)\n",
    "    context_mean_distance, context_closest_distance = context_stats(get_distances(vdb_instance))\n",
    "    context_statistics = {'Context Mean Permeability': [context_mean_perm],\n",
    "                    'Context Top 1 Permeability': [context_top_1_perm],\n",
    "                    'Context Mean Distance': [context_mean_distance],\n",
    "                    'Context Closest Distance': [context_closest_distance]\n",
    "                    }\n",
    "    return context_statistics\n",
    "\n",
    "def query_openai_model(vdb_instance, text):\n",
    "    prompt = template.format(context=get_context(vdb_instance), query=text)\n",
    "    openai_llm = ChatOpenAI(model_name=llm_model)\n",
    "    predicted_values = []\n",
    "    responses = []\n",
    "    \n",
    "    for i in range(num_llm_calls):\n",
    "        response = openai_llm([HumanMessage(content=prompt)])\n",
    "        token_stats = response.response_metadata['token_usage']\n",
    "        predicted_value = extract_value(response.content)\n",
    "        predicted_values.append(predicted_value)\n",
    "        responses.append(response.content)\n",
    "\n",
    "    n_input_tokens = token_stats['prompt_tokens']\n",
    "    n_of_examples = context_window\n",
    "    data = {'Predicted Values': [predicted_values],\n",
    "            'Number of samples': [n_of_examples],\n",
    "            'Prompt tokens': [n_input_tokens]     \n",
    "            }\n",
    "    return data\n",
    "\n",
    "def query_randomizer(vdb_instance):\n",
    "    global llm_model\n",
    "    llm_model = 'random'\n",
    "    predicted_values = []\n",
    "    for i in range(num_llm_calls):\n",
    "        predicted_values.append(random.choice(context_values(vdb_instance)))\n",
    "    n_of_examples = context_window\n",
    "    data = {'Predicted Values': [predicted_values],\n",
    "        'Number of samples': [n_of_examples]  \n",
    "        }\n",
    "    return data\n",
    "\n",
    "def llm_output_metrics(predicted_values, ground_truth):\n",
    "    mean_llm_response = round(statistics.fmean(predicted_values),2)\n",
    "    closest_guess = find_closest(ground_truth, predicted_values)\n",
    "\n",
    "    std = round(np.std(predicted_values), 4) \n",
    "    error_mean = calculate_errors(ground_truth, mean_llm_response)\n",
    "    error_closest = calculate_errors(ground_truth, closest_guess)\n",
    "    acc_mean = calculate_accuracy(ground_truth, mean_llm_response)\n",
    "    acc_closest = calculate_accuracy(ground_truth, closest_guess)\n",
    "\n",
    "    llm_metrics = {\n",
    "        'Mean LLM Response': [mean_llm_response],\n",
    "        'Closest Guess': [closest_guess],\n",
    "        'Standard Deviation': [std],\n",
    "        'Error Mean': [error_mean],\n",
    "        'Error Closest': [error_closest],\n",
    "        'Accuracy Mean': [acc_mean],\n",
    "        'Accuracy Closest': [acc_closest]\n",
    "        }\n",
    "    return llm_metrics\n",
    "\n",
    "def setup_parameters():\n",
    "    return {'Radius': [radius],\n",
    "        'nBits': [nBits],\n",
    "        'Model': [llm_model]}\n",
    "\n",
    "\n",
    "def run_evaluation(row, type_of_model='gpt'):\n",
    "    initial_data = extract_initial_data(row)\n",
    "    vdb = vdb_initialization(initial_data['text'][0])\n",
    "    if type_of_model == 'gpt':\n",
    "        llm_output = query_openai_model(vdb, initial_data['text'][0])\n",
    "    elif type_of_model == 'random':\n",
    "        print('random model is used')\n",
    "        llm_output = query_randomizer(vdb)\n",
    "    context_metr = context_metrics(vdb)\n",
    "    llm_response_metrics = llm_output_metrics(llm_output['Predicted Values'][0], initial_data['ground_truth'][0])\n",
    "    setup_params = setup_parameters()\n",
    "    final_dict = initial_data | context_metr | llm_output | llm_response_metrics | setup_params\n",
    "    return pd.DataFrame(final_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(df, type_of_model, type_of_test):\n",
    "    global manager\n",
    "    manager = DataFrameManager()\n",
    "    manager.add_texts_to_collection(train_set, 'Formatted_String', ['Formatted_String'])\n",
    "    df_list = []\n",
    "    for index, row in df.iterrows():\n",
    "        df_list.append(run_evaluation(row, type_of_model=type_of_model))\n",
    "    manager.chroma_client.delete_collection(name='chemical_data_collection')\n",
    "    del manager\n",
    "    return pd.concat(df_list).to_csv(f'../results/{llm_model}_{context_window}_{nBits}_{radius}_{type_of_test}.csv')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refining functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data added to collection successfully.\n",
      "random model is used\n",
      "random model is used\n"
     ]
    }
   ],
   "source": [
    "main(validation, type_of_model='random', type_of_test='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dspy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
